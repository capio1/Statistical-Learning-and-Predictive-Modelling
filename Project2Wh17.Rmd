---
title: "Wh17"
author: "Rosebella Capio"
date: "10/24/2018"
output: html_document
---




```{r}

library(corrplot)
library(plotly)
library(mclust)
library(NbClust)
library(tidyverse)
library(factoextra)
library(cluster)
library(NbClust)
library(fpc)
library(dendroextras)
library(dendextend)
library(mclust)
library(dbscan)
library(dplyr)
library(maps)
library(ggplot2)
library(ggthemes)
library(reshape2)
library(stringr)
library(FactoMineR)
require(magrittr)
require(useful)
library(mclust)
require(fastcluster)

wh17=read.csv("/Users/capio/Dropbox/SCHOOL/FALL 2018/STAT 517/Stat517-master/Data/World_Happiness_2017.csv")
#data preparation
is.na(wh17) #no missing values

#subsetting the data
country2=wh17$Country
score2=wh17$Happiness.Score
subset2=wh17[,7:12]
wh17.new=cbind(country2,score2,subset2)

#Scaling the data
wh17.scaled <- scale(wh17[, 6:12])
summary(wh17.scaled)

#Heat Map of Correlation Matrix
qplot(x=Var1, y=Var2, data=melt(cor(wh17.scaled, use="p")), fill=value, geom="tile") +
  scale_fill_gradient2(limits=c(-1, 1)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(title="Heatmap of Correlation Matrix", 
       x=NULL, y=NULL)

#Scaling the data
wh17.scaled <- scale(wh17[, 7:12])
summary(wh17.scaled)



# 1. PCA
wh17.scaled.pca <- PCA(wh17.scaled, graph=FALSE)
print(wh17.scaled.pca)

eigenvalues1 <- wh17.scaled.pca$eig
head(eigenvalues1)
#The proportion of variation retained by the principal components was extracted above.
#eigenvalues is the amount of variation retained by each PC. The first PC corresponds to the #maximum amount of variation in the data set. In this case, the first two principal #components are worthy of consideration because A 

fviz_screeplot(wh17.scaled.pca, addlabels = TRUE, ylim = c(0, 65))
#The scree plot shows us which components explain most of the variability in the data. In #this case, almost 80% of the variances contained in the data are retained by the first two #principal components.

head(wh17.scaled.pca$var$contrib)
#Variables that are correlated with PC1 and PC2 are the most important in explaining the variability in the data set.
#The contribution of variables was extracted above: The larger the value of the #contribution, the more the variable contributes to the component.

fviz_pca_var(wh17.scaled.pca, col.var="contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE  )
#This highlights the most important variables in explaining the variations retained by the #principal components.

#Using Pam Clustering Analysis to group countries
#finding the best number of k
number1 <- NbClust(wh17.scaled,distance="euclidean",
                  min.nc=2, max.nc=15, method='ward.D', index='all', alphaBeale = 0.1)

#according to majority grouping, the best number of k=3

#therefore using k=3
set.seed(44444)
pam2 <- pam(wh17.scaled, diss=FALSE, 3, keep.data=TRUE)
fviz_silhouette(pam2)

#Number of countries in each cluster
wh17.new$country2[pam2$id.med]

#This prints out one typical country represents each cluster.
fviz_cluster(pam2, stand = FALSE, geom = "point",
             ellipse.type = "norm")



#2. KMEANS CLUSTERING
km2 <- kmeans(wh17[, 6:12],3, iter.max=100)
## list of cluster assignments
order2=order(km2$cluster)
data.frame(wh17$Country[order2],km2$cluster[order2])

#Plot
plot(wh17.new$score2, wh17.new$Economy..GDP.per.Capita., type="n", xlim=c(3,19), xlab="HappinessScore", ylab="GDP")
text(x=wh17.new$score2, y=wh17.new$Economy..GDP.per.Capita., labels=wh17.new$country2,col=km2$cluster+1)


## same analysis, but now with clustering on all
## protein groups change the number of clusters to 5
set.seed(800)
grpVar1 <- kmeans(wh17.new[,-1], centers=3, nstart=10)
order2=order(grpVar1$cluster)
data.frame(wh17.new$country2[order2],grpVar1$cluster[order2])

#2D representation of Cluster solution

#Grouping clusters
g111 <- wh17.new[km2$cluster == 1,]$score
g222 <- wh17.new[km2$cluster == 2,]$score
g333 <- wh17.new[km2$cluster == 3,]$score

#histogram showing the distribution of the clusters
hist(g111, xlim=c(0,10), col=rgb(1,0,0,0.5), breaks=seq(0.25,10,0.25)  
     , main = "Histogram of Happiness Score for 3 cluster-groups"
     , xlab = "Country Happiness Score")
hist(g222, xlim=c(0,10), col=rgb(0,1,0,0.5), breaks=seq(0.25,10,0.25), add=T)
hist(g333, xlim=c(0,10), col=rgb(0,0,1,0.5), breaks=seq(0.25,10,0.25), add=T)
legend("topright", c("Group1", "Group2", "Group3")
       , fill=c(rgb(1,0,0,0.5),rgb(0,1,0,0.5),rgb(0,0,1,0.5)) )

#countries with top happiness rank
top2 <- which.max(c(mean(g111),mean(g222), mean(g333))) # which is the top group
happiest2 <- wh17.new[km2$cluster == top2, 1:2]
print(happiest2[order(happiest2$score, decreasing=TRUE), ], row.names = FALSE)


#3.HIERARCHICAL CLUSTERING (Dendogram)
wh17.new.agg=agnes(wh17.new,diss=FALSE,metric="euclidian")
plot(wh17.new.agg, main='Dendrogram') 

#4. MODEL BASED CLUSTERING
#the first 4 PCAs explain 81% of the total variation
par(mfrow=c(1,3))
mt.fit11<-Mclust(wh17.scaled[,1:6])
summary(mt.fit11); mt.fit11$modelName ; mt.fit11$G
fviz_mclust(mt.fit11,"BIC",palette="jco")




```

