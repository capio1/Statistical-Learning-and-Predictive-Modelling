---
title: "Wh16"
author: "Rosebella Capio"
date: "10/24/2018"
output: html_document
---


```{r}
library(corrplot)
library(plotly)
library(mclust)
library(NbClust)
library(tidyverse)
library(factoextra)
library(cluster)
library(NbClust)
library(fpc)
library(dendroextras)
library(dendextend)
library(mclust)
library(dbscan)
library(dplyr)
library(maps)
library(ggplot2)
library(ggthemes)
library(reshape2)
library(stringr)
library(FactoMineR)
require(magrittr)
require(useful)
library(mclust)
require(fastcluster)

wh16=read.csv("/Users/capio/Dropbox/SCHOOL/FALL 2018/STAT 517/Stat517-master/Data/World_Happiness_2016.csv")
#data preparation
is.na(wh16) #no missing values

country=wh16$Country
score=wh16$Happiness.Score
subset=wh16[,7:13]
wh16.new=cbind(country,score,subset)

#Scaling the data
wh16.scaled <- scale(wh16[, 7:13])
summary(wh16.scaled)

#Heat Map of Correlation Matrix
qplot(x=Var1, y=Var2, data=melt(cor(wh16.scaled, use="p")), fill=value, geom="tile") +
  scale_fill_gradient2(limits=c(-1, 1)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(title="Heatmap of Correlation Matrix", 
       x=NULL, y=NULL)


# 1. PCA
wh16.scaled.pca <- PCA(wh16.scaled, graph=FALSE)
print(wh16.scaled.pca)

eigenvalues <- wh16.scaled.pca$eig
head(eigenvalues)
#The proportion of variation retained by the principal components was extracted above.
#eigenvalues is the amount of variation retained by each PC. The first PC corresponds to the #maximum amount of variation in the data set. In this case, the first two principal #components are worthy of consideration because A 

fviz_screeplot(wh16.scaled.pca, addlabels = TRUE, ylim = c(0, 65))
#The scree plot shows us which components explain most of the variability in the data. In #this case, almost 80% of the variances contained in the data are retained by the first two #principal components.

head(wh16.scaled.pca$var$contrib)
#Variables that are correlated with PC1 and PC2 are the most important in explaining the variability in the data set.
#The contribution of variables was extracted above: The larger the value of the #contribution, the more the variable contributes to the component.

fviz_pca_var(wh16.scaled.pca, col.var="contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE  )
#This highlights the most important variables in explaining the variations retained by the #principal components.

#Using Pam Clustering Analysis to group countries
#finding the best number of k
number <- NbClust(wh16.scaled,distance="euclidean",
                  min.nc=2, max.nc=15, method='ward.D', index='all', alphaBeale = 0.1)

#according to majority grouping, the best number of k=3

#therefore using k=3
set.seed(444)
pam1 <- pam(wh16.scaled, diss=FALSE, 3, keep.data=TRUE)
fviz_silhouette(pam1)

#Number of countries in each cluster
wh16.new$country[pam1$id.med]

#This prints out one typical country represents each cluster.
fviz_cluster(pam1, stand = FALSE, geom = "point",
             ellipse.type = "norm")



#2. KMEANS CLUSTERING
km1 <- kmeans(wh16[, 7:13],3, iter.max=100)
## list of cluster assignments
order=order(km1$cluster)
data.frame(wh16$Country[order],km1$cluster[order])

#Plot
plot(wh16$Happiness.Score, wh16$Economy..GDP.per.Capita., type="n", xlim=c(3,19), xlab="HappinessScore", ylab="GDP")
text(x=wh16$Happiness.Score, y=wh16$Economy..GDP.per.Capita., labels=wh16$Country,col=km1$cluster+1)

## same analysis, but now with clustering on all
## protein groups change the number of clusters to 7
set.seed(800)
grpVar <- kmeans(wh16.new[,-1], centers=3, nstart=10)
order1=order(grpVar$cluster)
data.frame(wh16.new$country[order1],grpVar$cluster[order1])

#2D representation of Cluster solution
clusplot(wh16.new[,-1], grpVar$cluster, main='2D representation of the Cluster solution', color=TRUE, shade=TRUE, labels=2, lines=0)

#Grouping clusters
g11 <- wh16.new[km1$cluster == 1,]$score
g22 <- wh16.new[km1$cluster == 2,]$score
g33 <- wh16.new[km1$cluster == 3,]$score

#histogram showing the distribution of the clusters
hist(g11, xlim=c(0,10), col=rgb(1,0,0,0.5), breaks=seq(0.25,10,0.25)  
     , main = "Histogram of Happiness Score for 3 cluster-groups"
     , xlab = "Country Happiness Score")
hist(g22, xlim=c(0,10), col=rgb(0,1,0,0.5), breaks=seq(0.25,10,0.25), add=T)
hist(g33, xlim=c(0,10), col=rgb(0,0,1,0.5), breaks=seq(0.25,10,0.25), add=T)
legend("topright", c("Group1", "Group2", "Group3")
       , fill=c(rgb(1,0,0,0.5),rgb(0,1,0,0.5),rgb(0,0,1,0.5)) )

#countries with top happiness rank
top1 <- which.max(c(mean(g11),mean(g22), mean(g33))) # which is the top group
happiest1 <- wh16.new[km1$cluster == top1, 1:2]
print(happiest1[order(happiest1$score, decreasing=TRUE), ], row.names = FALSE)


#3.HIERARCHICAL CLUSTERING (Dendogram)
wh16.new.agg=agnes(wh16.new,diss=FALSE,metric="euclidian")
plot(wh16.new.agg, main='Dendrogram') 

# cut tree into 3 clusters
#groups <- cutree(wh16.new.agg, k=3)
#rect.hclust(wh16.new.agg, k=3, border="red") 



#4. MODEL BASED CLUSTERING
#the first 4 PCAs explain 81% of the total variation
par(mfrow=c(1,3))
mt.fit1<-Mclust(wh16.scaled[,1:7])
summary(mt.fit1); mt.fit1$modelName ; mt.fit1$G
fviz_mclust(mt.fit1,"BIC",palette="jco")

```

